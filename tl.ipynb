{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformer_lens\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model and doing some stuff to make it work properly with TransformerLens. I have to run the model in 16-bit precision otherwise I run out of memory later on.\n",
    "\n",
    "In this notebook I filter out all prompts that are longer than 1600 tokens due to memory constraints. If you want to run this with longer reasoning traces, you may need to modify TransformerLens, which sets the context length for Qwen2.5 to 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-1.5B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_hf = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model = transformer_lens.HookedTransformer.from_pretrained_no_processing(\"Qwen/Qwen2.5-1.5B\", hf_model=model_hf, tokenizer=tokenizer_hf, device=device, dtype=torch.bfloat16)\n",
    "\n",
    "# The rotary base used in the base qwen model is apparently different from the one in the r1 distilled.\n",
    "# There's a PR in the transformerlens repo to get this value from the hf model config, but I don't want\n",
    "# to bother with using a modified version. So I'm just fixing everything after the fact.\n",
    "\n",
    "model.cfg.rotary_base = model_hf.config.rope_theta\n",
    "\n",
    "for block in model.blocks:\n",
    "    attn = block.attn\n",
    "    sin, cos = attn.calculate_sin_cos_rotary(\n",
    "        model.cfg.rotary_dim,\n",
    "        model.cfg.n_ctx,\n",
    "        base=model.cfg.rotary_base,\n",
    "        dtype=model.cfg.dtype,\n",
    "    )\n",
    "    attn.register_buffer(\"rotary_sin\", sin.to(device))\n",
    "    attn.register_buffer(\"rotary_cos\", cos.to(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the dataset and generating the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"immindich/reasoning-steps-labeled\")\n",
    "from util import reconstruct_prompt\n",
    "import random\n",
    "\n",
    "min_step_size = 10\n",
    "max_prompt_size = 1600\n",
    "\n",
    "def get_step_data(key):\n",
    "    step_prompts = []\n",
    "    steps = []\n",
    "\n",
    "    for problem in dataset[\"train\"].shuffle(seed=42):\n",
    "        if problem[key]:\n",
    "            step = random.choice(problem[key])\n",
    "            prompt = model.tokenizer.encode(reconstruct_prompt(model.tokenizer, problem, step), return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "            step_tokens = model.tokenizer.encode(problem[\"steps\"][step], return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "            if step_tokens.shape[1] < min_step_size or prompt.shape[1] > max_prompt_size:\n",
    "                continue\n",
    "            step_prompts.append(prompt)\n",
    "            steps.append(step_tokens)\n",
    "\n",
    "    return step_prompts, steps\n",
    "\n",
    "conclusion_prompts, conclusion_steps = get_step_data(\"conclusion_steps\")\n",
    "verification_prompts, verification_steps = get_step_data(\"verification_steps\")\n",
    "\n",
    "test_size = 50\n",
    "conclusion_prompts_test = conclusion_prompts[:test_size]\n",
    "conclusion_steps_test = conclusion_steps[:test_size]\n",
    "verification_prompts_test = verification_prompts[:test_size]\n",
    "verification_steps_test = verification_steps[:test_size]\n",
    "\n",
    "conclusion_prompts_train = conclusion_prompts[test_size:]\n",
    "conclusion_steps_train = conclusion_steps[test_size:]\n",
    "verification_prompts_train = verification_prompts[test_size:]\n",
    "verification_steps_train = verification_steps[test_size:]\n",
    "\n",
    "train_size_conclusion = len(conclusion_prompts_train)\n",
    "train_size_verification = len(verification_prompts_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we collect the model activations across all layers on the first five tokens of each conclusion/verification step in the training set. This process is actually pretty quick, but it made me run out of memory before I switched to bfloat16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08949c9dc4e049318165e81e33124e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f40aad86024085a790ab9856dbdaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step_tokens = 5\n",
    "\n",
    "activations_conclusion = torch.zeros(train_size_conclusion, model.cfg.n_layers, step_tokens, model.cfg.d_model, device=device)\n",
    "activations_verification = torch.zeros(train_size_verification, model.cfg.n_layers, step_tokens, model.cfg.d_model, device=device)\n",
    "\n",
    "def layer_activations_hook(layer, prompt, step_start, activations):\n",
    "    def hook_fn(value, hook):\n",
    "        # value: batch, pos, d_model\n",
    "        activations[prompt][layer] = value[0, step_start:step_start+step_tokens, :]\n",
    "\n",
    "    return hook_fn\n",
    "\n",
    "def collect_activations(prompts, steps, activations):\n",
    "    for i, (prompt, step) in tqdm(list(enumerate(zip(prompts, steps)))):\n",
    "        hooks = [(f'blocks.{layer}.hook_resid_pre', layer_activations_hook(layer, i, prompt.shape[1], activations)) for layer in range(model.cfg.n_layers)]\n",
    "        with model.hooks(hooks):\n",
    "            model(torch.cat([prompt, step], dim=1))\n",
    "\n",
    "collect_activations(verification_prompts_train, verification_steps_train, activations_verification)\n",
    "collect_activations(conclusion_prompts_train, conclusion_steps_train, activations_conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_activations_conclusion = activations_conclusion.mean(dim=0)\n",
    "mean_activations_verification = activations_verification.mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, the difference of the two means should be a vector that causes the model to output a conclusion instead of a verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4d3985d7a84f0db6c0030633582fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Let me just recap the steps to make sure I', 'Let me just double-check to make sure I didn', 'Let me just verify that step-by-step to make', 'Let me just double-check my steps to make sure', 'Let me just recap the steps to make sure I'], ['Let me just go through that again to make sure', 'Let me just recap the steps to make sure I', \"Let me just verify that I didn't make a\", \"Let me just recap to make sure I didn't\", 'Let me just recap the steps to make sure I'], ['Let me just double-check my steps to make sure', 'Let me just double-check my steps to make sure', 'Let me just recap the steps to make sure I', \"Let me just verify that I didn't make any\", 'Let me just verify my steps to make sure I'], ['Let me just double-check to make sure I didn', \"Let me double-check to make sure I didn't\", 'Let me just double-check to make sure I didn', \"But just to make sure I didn't make any\", 'Let me just double-check my reasoning. Each logarith'], ['Let me just recap the steps to make sure I', 'Let me just recap the steps to make sure I', 'Let me just double-check to make sure I didn', 'Let me just double-check my steps to make sure', \"But just to make sure I didn't make any\"]], [['Wait, let me just double-check my steps to', \"Let me just recap to make sure I didn't\", \"Let me double-check to make sure I didn't\", 'Let me just double-check my steps to make sure', \"Let me just recap to make sure I didn't\"], ['Let me just verify this once more to make sure', \"Let me just verify that I didn't make any\", 'Let me just recap the steps:\\n\\n1. Recogn', 'Let me just double-check my steps to make sure', 'Let me just recap the steps to make sure I'], [\"Let me just recap to make sure I didn't\", 'Let me just double-check to make sure I didn', 'Let me just double-check my steps to make sure', \"Wait, let me just make sure I didn't\", 'Let me just double-check my steps to make sure'], [\"Wait, just to make sure I didn't make\", \"Let me just recap to make sure I didn't\", \"But just to make sure I didn't make a\", 'Let me double-check my steps to make sure I', 'Let me double-check my steps to make sure I'], ['Let me just double-check my steps to make sure', 'Let me just verify this step again to make sure', \"Wait, let me just make sure I didn't\", \"Just to make sure I didn't make a mistake\", \"Let me just recap to make sure I didn't\"]], [['I guess the key takeaway here is recognizing that when', 'Let me just double-check to make sure I didn', 'Let me just double-check my steps to make sure', 'Let me just double-check my steps to make sure', 'Let me just double-check my steps to make sure'], ['Let me just recapitulate the steps to make', 'Wait, let me double-check that. Starting from', \"Let me double-check that I didn't make any\", 'Wait, let me just verify that again step by', 'Let me double-check that. Starting with the product'], ['Wait, let me just recap to make sure I', 'Let me just verify that once again to make sure', 'Wait, let me just double-check my steps to', 'Let me just double-check my steps to make sure', 'Let me just double-check my steps to make sure'], ['Let me just double-check to make sure I didn', 'Let me just verify that again step by step to', \"Let me just recap to make sure I didn't\", 'Let me just double-check my steps to make sure', 'Let me double-check my steps to make sure I'], ['Let me just verify each step again to make sure', \"Let me just recap to make sure I didn't\", 'But let me double-check to make sure I didn', 'Let me just double-check my steps to make sure', 'Let me just double-check my steps to make sure']], [['Let me just double-check my steps to make sure', 'Wait, let me just double-check that. Starting', \"Let me just recap to make sure I didn't\", 'Let me just recap the steps to make sure I', 'Let me just double-check my steps to make sure'], ['Let me just double-check my steps to make sure', 'Wait, let me double-check to make sure I', 'Wait, let me double-check that to make sure', 'Just to double-check, let me recap:\\n\\n1', 'Let me just recap the steps to make sure I'], [\"But just to make sure I didn't make any\", 'Let me just double-check my steps to make sure', 'Let me double-check my steps to make sure I', 'Let me just double-check my steps to make sure', \"Let me just recap to make sure I didn't\"], [\"Let me double-check to make sure I didn't\", \"Let me double-check to make sure I didn't\", \"But just to make sure I didn't make a\", \"Wait, let me just make sure I didn't\", \"Let me double-check that I didn't make any\"], ['Wait, let me just double-check that. So', \"But just to make sure I didn't make a\", \"Let me just recap to make sure I didn't\", \"Let me just recap to make sure I didn't\", 'Wait, let me just verify that step again where']], [[\"Let me double-check to make sure I didn't\", 'Let me just double-check my steps to make sure', 'Let me just double-check my steps to make sure', 'Let me double-check my steps to make sure I', 'Let me double-check my steps to make sure I'], ['Let me just double-check my steps to make sure', 'Let me just recap the steps:\\n\\n1. Convert', 'Let me just double-check my steps to make sure', 'Let me just double-check my steps to make sure', 'Wait, let me double-check that. Starting with'], ['Wait, let me just verify that step again to', \"But just to make sure I didn't make a\", 'Wait, let me just double-check that. So', 'Let me just double-check my steps to make sure', 'Wait, let me just recap to make sure I'], ['Let me just recapitulate the steps to make', 'Wait, let me just go through that again to', 'Let me double-check my reasoning. Each logarithm', 'But let me double-check to make sure I didn', 'Let me just double-check my reasoning. Each logarith'], ['Let me just recap the steps to make sure I', 'Let me just double-check to make sure I didn', \"Let me just recap to make sure I didn't\", \"Let me just recap to make sure I didn't\", \"Let me just recap to make sure I didn't\"]], [['Let me just double-check my steps to make sure', \"Let me just recap to make sure I didn't\", 'Just to recap the steps:\\n\\n1. Recognized', \"Let me double-check to make sure I didn't\", \"Wait, just to make sure I didn't make\"], ['Let me just recap the steps:\\n\\n1. Recogn', 'Let me just recap the steps to make sure I', 'Wait, let me just verify that step-by-step', 'Let me just double-check my reasoning to make sure', 'Let me just go through that again step by step'], ['Let me just double-check that each step makes sense', 'Let me just recap the steps:\\n\\n1. Recogn', 'Let me just double-check that. Each logarithm', 'Just to recap, I converted each logarithm into', \"Let me just recap to make sure I didn't\"], ['Let me just verify each step again to make sure', 'Let me double-check that. Each logarithm canc', 'Let me double-check that logic. Each logarithm', 'Wait, let me double-check that. So,', 'Let me double-check my steps. I converted each'], [\"Let me just recap to make sure I didn't\", 'Let me just recap what I did. I converted', 'Wait, let me just double-check my steps here', 'Let me just recap the steps:\\n\\n1. Recogn', 'Let me just recap:\\n\\nEach logarithm term canc']], [[\"I think that's the right answer. Let me\", 'Just to recap the steps:\\n\\n1. Applied the', 'Let me just double-check my steps to make sure', 'Just to double-check, each step of the cancellation', 'Let me just double-check my steps to make sure'], [\"But just to make sure I didn't make a\", 'Let me just verify that again step by step.', 'Let me double-check that. Each logarithm canc', 'Wait, let me just double-check that. Each', 'Let me just double-check my reasoning. Each logarith'], ['Wait, let me just double-check to make sure', 'Let me just double-check my steps to make sure', 'Let me just double-check my steps to make sure', 'Let me just double-check my steps. I started', \"But just to make sure I didn't make any\"], ['Let me just double-check my steps to make sure', 'Wait, let me just make sure I did that', \"Let me just make sure I didn't make any\", 'Let me double-check that. Each step cancels', 'Let me just double-check my steps to make sure'], ['Let me just double-check my steps:\\n\\n1.', 'Let me just double-check my steps to make sure', 'Let me just double-check my reasoning. Each logarith', 'Wait, let me double-check that. Each step', 'Just to recap:\\n\\n1. Recognize that each']], [[\"I think that's the correct answer, but just\", 'Let me just double-check my steps to make sure', 'Let me double-check my steps to make sure I', \"I think that's the correct answer. It's\", \"I think that's the correct answer. It was\"], ['Let me just double-check my reasoning. Each logarith', 'Let me just verify that each step is correct.', \"But wait, just to make sure I didn't\", 'Just to recap, the key steps were:\\n\\n1', 'Let me just recap the steps:\\n\\n1. Rewrite'], ['Let me just double-check my steps to make sure', 'Let me just recap the steps to make sure I', 'Let me just double-check that. Starting with the', 'Let me just double-check my reasoning. Each time', 'Let me just recap the steps:\\n\\n1. Recogn'], ['Let me just double-check my steps. I converted', 'But wait, let me just double-check that.', 'Let me just double-check my steps. I converted', 'Let me just recap the steps:\\n\\n1. Each', 'Let me just recapitulate the steps:\\n\\n1'], ['Let me just double-check my steps. I converted', 'Let me just recap the steps:\\n\\n1. Recogn', \"But just to make sure I didn't make a\", 'Let me just double-check that. Starting with the', 'Let me just double-check my steps. Each logarith']], [['I think I can confidently say that the value of', 'Just to recap:\\n\\n1. Rewrote each logarith', \"Just to double-check, let's recap the steps\", 'Just to recap:\\n\\n1. Recognized each term', 'Let me just double-check my steps to make sure'], [\"But just to make sure I didn't make any\", \"But just to make sure I didn't make a\", \"I think I've got it right. The value\", 'Wait, but let me just double-check that.', \"I think that's the answer. But just to\"], ['Let me just verify each step again to make sure', \"I think that's the correct answer. But let\", 'Let me just recap the steps:\\n\\n1. Recogn', \"But just to make sure I didn't make any\", 'Let me just recap the steps:\\n\\n1. Recogn'], ['Let me double-check that. Each time, the', 'Just to double-check, let me make sure I', 'But let me just verify that again. Let me', 'Let me just recap what I did: I converted', 'Let me just double-check to make sure I didn'], ['Let me just recap the steps:\\n\\n1. Recogn', 'I should double-check my reasoning here. Each step', \"I don't think I made any mistakes here,\", \"I think that's the final answer. Let me\", 'Let me just double-check my reasoning. Each logarith']], [[\"I think that's the correct answer. It makes\", '**Final Answer**\\nThe value of the given product', '**Final Answer**\\nThe value of the given product', 'Just to recap, I started by expressing each logarith', \"I think that's the answer then. It wasn\"], ['Let me just recap the steps:\\n\\n1. Recogn', 'Let me just verify this step by step. Starting', 'Let me just recap the steps:\\n\\n1. Rewrite', \"I think that's the answer. It's \", \"I think that's the right answer. It's\"], [\"I think that's the answer. Let me just\", 'Therefore, the value of the given expression is ', \"I think that's the answer. It makes sense\", 'Let me just recap what I did:\\n\\n1.', 'Let me just double-check my steps. I used'], [\"I think that's the answer. But let me\", \"I think that's the answer. It's a\", \"I think that's the answer then. It should\", \"I think that's the right answer. Let me\", 'Let me just recap the steps:\\n\\n1. Each'], ['Let me just recap the steps:\\n\\n1. Recogn', 'Wait, let me just double-check my steps.', \"I don't think I made any mistakes here.\", 'Let me just recap the steps:\\n\\n1. Recogn', 'Let me just recap the steps:\\n\\n1. Recogn']], [['**Final Answer**\\nThe value of the product is', \"I think that's the correct answer. It's\", '**Final Answer**\\nThe value of the given product', '**Final Answer**\\n\\\\boxed{3}\\n</think>', \"I think that's the correct answer. It's\"], [\"I think that's the right answer. Let me\", \"I think that's the right answer. Let me\", '**Final Answer**\\nThe value of the product is', \"I think that's the answer. Let me just\", '**Final Answer**\\nThe value of the product is'], [\"I think that's the solution. Let me just\", 'Let me just double-check that each step makes sense', \"I think that's the correct answer. Let me\", '**Final Answer**\\nThe value of the given expression', \"I think that's the answer. Let me just\"], ['**Final Answer**\\n\\\\boxed{3}\\n</think>', \"I think that's the answer. Let me just\", '**Final Answer**\\nThe value of the product is', '**Final Answer**\\nThe value of the product is', 'Let me just recap the steps:\\n\\n1. Recogn'], ['Let me just double-check my steps. I applied', 'Let me just double-check my steps. I converted', '**Final Answer**\\n\\\\boxed{3}\\n</think>', \"I think that's the answer. Let me just\", \"I think that's the answer. Let me double\"]], [[\"I think that's the correct answer. I don\", \"I think that's the correct answer. It makes\", 'Let me just double-check my steps to make sure', 'I think I can confidently say that the value of', \"I think that's the correct answer. It makes\"], [\"I think that's the answer. It's always\", \"I think that's the correct answer. It's\", \"I think that's the answer. Let me just\", \"I think that's the correct answer. It was\", \"I think that's the correct answer. It's\"], [\"I think that's the correct answer. It makes\", \"I think that's the answer. 3 is\", \"I think that's the correct answer. I don\", \"I think that's the correct answer. Let me\", \"I think I've got it right. The final\"], [\"I think that's the answer. It makes sense\", \"I think that's the correct answer. Let me\", \"I think that's the right answer. Let me\", \"I think that's the answer. 3 is\", \"I think that's the right answer. I don\"], ['Let me just recap the steps:\\n\\n1. Express', \"I think that's the answer. Let me just\", 'I think I did it right. Each step canc', \"I don't think I made a mistake here.\", \"I think that's the answer. It makes sense\"]], [[\"I think I've got it. The value of\", \"I don't think I made any mistakes here.\", 'Just want to double-check my steps:\\n\\n1.', \"I don't think I made any mistakes in the\", 'Just want to double-check my steps:\\n\\n1.'], [\"I think I've got it. The answer is\", \"I don't think I made any mistakes here.\", \"I don't think I made any mistakes here.\", \"I don't think I made any mistakes here.\", \"I don't think I made any mistakes here.\"], [\"I think that's the correct answer. It makes\", 'I think I got it. The value of the', \"I think that's the correct answer. Let me\", 'Let me just recap the steps:\\n\\n1. Recogn', 'I think I got it. The value of the'], [\"I don't think I need to do any more\", \"I think that's the answer then. 3\", 'I think I got it right. It was a', \"I think that's the correct answer. I don\", \"I think that's the right answer. I don\"], ['Let me just recap the steps:\\n\\n1. Recogn', \"I think that's the right answer. Let me\", \"I think that's the answer. It all comes\", \"I think that's the answer. It makes sense\", 'Let me just double-check my steps. I applied']], [[\"I think that's the answer. It feels good\", \"I think that's the final answer. I don\", \"I think I've got it. The answer should\", 'I think I can confidently say that the value of', 'I think this makes sense. Each term cancels'], [\"I think that's the answer. It's always\", \"I think that's the correct answer. Each logarith\", 'I think that makes sense. The key here was', 'I think the key here is recognizing that each logarith', \"I think that's the correct answer. I don\"], ['I think that makes sense. Each step cancels', \"I think that's the correct answer. Each logarith\", \"I think that's the correct answer. Each step\", \"I think that's the correct answer. I don\", \"I think I've got it then. The final\"], [\"I think that's the correct answer. It's\", \"I think that's the answer. Let me just\", \"I think that's the correct answer. Each step\", \"I don't think I made any mistakes here.\", \"I think that's the answer. It makes sense\"], [\"I think that's the answer. Let me just\", \"I think that's the right answer. It's\", \"I think that's the answer. It's always\", \"I think that's the answer then. It makes\", \"I think that's the right answer. It makes\"]], [['Just to recap the steps:\\n\\n1. Applied the', 'Just to recap, the key steps were:\\n\\n1', \"I think I'm confident that the value of the\", 'Just to recap, each logarithm cancels out', 'Just to recap the steps:\\n\\n1. Converted each'], [\"I think that's the correct answer. It's\", \"I think that's the correct answer. The key\", \"I think that's the correct answer. The key\", \"I don't think I made any mistakes here.\", 'I feel confident that the answer is 3.'], [\"I think that's the correct answer. I don\", \"I think that's the correct answer, but just\", 'Therefore, the value of the product is 3', \"I don't think I made any mistakes in the\", \"I think that's the answer. The key was\"], [\"I think that's the correct answer. It makes\", 'Therefore, the value of log₂3 multiplied by', \"I think that's the answer. But let me\", \"I think that's the correct answer. But let\", \"I think that's the answer, 3.\"], [\"I think that's the correct answer, but let\", 'But let me double-check my reasoning. Each step', \"I think I've got it then. The value\", \"I think I've got it then. The value\", \"I think that's the answer then. The value\"]]]\n"
     ]
    }
   ],
   "source": [
    "tokens = 10\n",
    "\n",
    "feature_vectors = mean_activations_conclusion - mean_activations_verification\n",
    "\n",
    "samples = []\n",
    "\n",
    "# I only checked layers 5-20 because an earlier version of this was taking a while to run. But I expect that we want something from the middle layers anyway.\n",
    "for layer in tqdm(range(5, 20)):\n",
    "    samples_layer = []\n",
    "    for pos in range(step_tokens):\n",
    "        samples_pos = []\n",
    "        def intervene_hook(value, hook):\n",
    "            value += feature_vectors[layer, pos, :]\n",
    "            return value\n",
    "        for i in range(5):\n",
    "            with model.hooks([(f\"blocks.{layer}.hook_resid_pre\", intervene_hook)]):\n",
    "                output = model.generate(verification_prompts_test[0], max_new_tokens=tokens, top_p=0.95, temperature=0.6, verbose=False)\n",
    "                samples_pos.append(model.tokenizer.decode(output[0, -tokens:]))\n",
    "        samples_layer.append(samples_pos)\n",
    "    samples.append(samples_layer)\n",
    "\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = feature_vectors[11][1]\n",
    "\n",
    "def intervene_hook(value, hook):\n",
    "    value += feature_vector\n",
    "    return value\n",
    "\n",
    "samples_intervened = []\n",
    "samples_control = []\n",
    "\n",
    "with model.hooks([(f\"blocks.{11}.hook_resid_pre\", intervene_hook)]):\n",
    "    for prompt in verification_prompts_test:\n",
    "        for j in range(5):\n",
    "            output = model.generate(prompt, max_new_tokens=tokens, top_p=0.95, temperature=0.6, verbose=False)\n",
    "            samples_intervened.append(model.tokenizer.decode(output[0, -tokens:]))\n",
    "\n",
    "for prompt in verification_prompts_test:\n",
    "    for j in range(5):\n",
    "        output = model.generate(prompt, max_new_tokens=tokens, top_p=0.95, temperature=0.6, verbose=False)\n",
    "        samples_control.append(model.tokenizer.decode(output[0, -tokens:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.messages.batch_create_params import Request\n",
    "def create_batch_requests(completions):\n",
    "    batch_requests = []\n",
    "    for j, completion in enumerate(completions):            \n",
    "        user_prompt = f\"\"\"\n",
    "        Attached to this prompt is the output of a language model which reasons step by step by step. The output given to you is the beginning of one of the reasoning steps. Classify the output into one of the following categories.\n",
    "\n",
    "        1. The model is concluding that it has figured out the final answer. In this case, the output should start with something like \"I think I'm confident\" or \"Therefore the answer is\" or \"I don't see any mistakes here\".\n",
    "\n",
    "        2. The model is verifying its work. In this case, the output should start with something like \"Wait, but let me make sure,\" or \"Let me double-check,\" or \"Wait, to make sure\".\n",
    "\n",
    "        3. The output does not seem to fit into the first two categories, or it is not clear what the model is doing.\n",
    "        \n",
    "        Do not respond with anything other than the number of the category. Note that the output may not be a complete sentence, because the model was only asked to generate the first 10 tokens.\n",
    "        \n",
    "        Here is the output of the language model:\n",
    "        \n",
    "        {completion}\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_requests.append(Request(\n",
    "            custom_id = f\"{j}\",\n",
    "            params=MessageCreateParamsNonStreaming(\n",
    "                model=\"claude-3-7-sonnet-20250219\",\n",
    "                max_tokens=100,\n",
    "                temperature=1,\n",
    "                system=\"You are a helpful assistant labeling data generated by a language model\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": user_prompt\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    return batch_requests\n",
    "\n",
    "requests_control = create_batch_requests(samples_control)\n",
    "requests_intervened = create_batch_requests(samples_intervened)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import anthropic\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: this next cell costs money (but not very much)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MessageBatch(id='msgbatch_0115oRBZyvBrNTpvL3wtFGkS', archived_at=None, cancel_initiated_at=None, created_at=datetime.datetime(2025, 3, 9, 8, 30, 47, 205122, tzinfo=datetime.timezone.utc), ended_at=None, expires_at=datetime.datetime(2025, 3, 10, 8, 30, 47, 205122, tzinfo=datetime.timezone.utc), processing_status='in_progress', request_counts=MessageBatchRequestCounts(canceled=0, errored=0, expired=0, processing=250, succeeded=0), results_url=None, type='message_batch')\n",
      "MessageBatch(id='msgbatch_0184PgSsS95e7bzsdH8pH2mY', archived_at=None, cancel_initiated_at=None, created_at=datetime.datetime(2025, 3, 9, 8, 30, 47, 576690, tzinfo=datetime.timezone.utc), ended_at=None, expires_at=datetime.datetime(2025, 3, 10, 8, 30, 47, 576690, tzinfo=datetime.timezone.utc), processing_status='in_progress', request_counts=MessageBatchRequestCounts(canceled=0, errored=0, expired=0, processing=250, succeeded=0), results_url=None, type='message_batch')\n"
     ]
    }
   ],
   "source": [
    "batch_control = client.messages.batches.create(requests=requests_control)\n",
    "batch_intervened = client.messages.batches.create(requests=requests_intervened)\n",
    "\n",
    "print(batch_control)\n",
    "print(batch_intervened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control: 0.088\n",
      "Intervened: 0.336\n"
     ]
    }
   ],
   "source": [
    "results_control = list(client.messages.batches.results(\n",
    "    batch_control.id,\n",
    "))\n",
    "\n",
    "results_intervened = list(client.messages.batches.results(\n",
    "    batch_intervened.id,\n",
    "))\n",
    "\n",
    "control_numbers = list(map(lambda x: int(x.result.message.content[0].text), results_control))\n",
    "intervened_numbers = list(map(lambda x: int(x.result.message.content[0].text), results_intervened))\n",
    "\n",
    "def proportion_equal(numbers, value):\n",
    "    # return the proportion of numbers that are equal to value\n",
    "    return sum(1 for number in numbers if number == value) / len(numbers)\n",
    "\n",
    "print(\"Control:\", proportion_equal(control_numbers, 1))\n",
    "print(\"Intervened:\", proportion_equal(intervened_numbers, 1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
